<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>URL of Some Top Conferences’ papers</title>
    <url>/2021/08/29/URL%20of%20Some%20Top%20Conferences%E2%80%99%20papers/</url>
    <content><![CDATA[<p>URL of Some Top Conferences’ papers (NIPS, CVPR, AAAI, IJCAI, MM…)<br><span id="more"></span></p>
<ol>
<li><h6 id="NIPS-1987"><a href="#NIPS-1987" class="headerlink" title="NIPS(1987-)"></a>NIPS(1987-)</h6><h6 id="Address-https-proceedings-neurips-cc-paper"><a href="#Address-https-proceedings-neurips-cc-paper" class="headerlink" title="Address = https://proceedings.neurips.cc/paper/"></a>Address = <a href="https://proceedings.neurips.cc/paper/">https://proceedings.neurips.cc/paper/</a></h6></li>
<li><h6 id="AAAI-1980"><a href="#AAAI-1980" class="headerlink" title="AAAI(1980-)"></a>AAAI(1980-)</h6><p>Address = <a href="https://www.aaai.org/Library/AAAI/aaai-library.php">https://www.aaai.org/Library/AAAI/aaai-library.php</a></p>
</li>
<li><h6 id="CVPR、ICCV、WACV、ECCV、ACCV-2013"><a href="#CVPR、ICCV、WACV、ECCV、ACCV-2013" class="headerlink" title="CVPR、ICCV、WACV、ECCV、ACCV(2013-)"></a>CVPR、ICCV、WACV、ECCV、ACCV(2013-)</h6><p>Address = <a href="https://openaccess.thecvf.com/menu">https://openaccess.thecvf.com/menu</a></p>
</li>
<li><h6 id="ICML、ACML、COLT-2013"><a href="#ICML、ACML、COLT-2013" class="headerlink" title="ICML、ACML、COLT(2013-)"></a>ICML、ACML、COLT(2013-)</h6><p><a href="http://proceedings.mlr.press/index.html">http://proceedings.mlr.press/index.html</a></p>
</li>
<li><h6 id="ICLR-2018"><a href="#ICLR-2018" class="headerlink" title="ICLR(2018-)"></a>ICLR(2018-)</h6><p><a href="https://openreview.net/group?id=ICLR.cc/{Year}/Conference">https://openreview.net/group?id=ICLR.cc/{Year}/Conference</a></p>
</li>
<li><h6 id="IJCAI"><a href="#IJCAI" class="headerlink" title="IJCAI"></a>IJCAI</h6><p>Address = <a href="https://www.ijcai.org/proceedings/{Year}/">https://www.ijcai.org/proceedings/{Year}/</a></p>
</li>
<li><h6 id="ACM-Digital-Library-MM-etc"><a href="#ACM-Digital-Library-MM-etc" class="headerlink" title="ACM Digital Library(MM, etc.)"></a>ACM Digital Library(MM, etc.)</h6><p>If the download fails(Mostly), go to 8..</p>
</li>
<li><h6 id="Awesome-Tool-for-Papers"><a href="#Awesome-Tool-for-Papers" class="headerlink" title="Awesome Tool for Papers"></a>Awesome Tool for Papers</h6><ol>
<li><p><strong>Sci-hub</strong>: <a href="https://www.sci-hub.ren/">https://www.sci-hub.ren/</a></p>
</li>
<li><p><strong>SPIS</strong> (&gt;=SCi-hub): <a href="http://spis.hnlat.com/">http://spis.hnlat.com/</a></p>
</li>
<li><p><strong>ResearchGate</strong>: <a href="https://www.researchgate.net/">https://www.researchgate.net/</a></p>
</li>
<li><p><strong>Google Scholar</strong>: <a href="https://scholar.google.com/">https://scholar.google.com/</a></p>
</li>
<li><p><strong>Arxiv:</strong> <a href="https://arxiv.org/">https://arxiv.org/</a></p>
</li>
<li><p><strong>DBLP:</strong> <a href="https://dblp.uni-trier.de/">https://dblp.uni-trier.de/</a></p>
</li>
</ol>
</li>
</ol>
<h6 id=""><a href="#" class="headerlink" title=" "></a> </h6><h6 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h6><h6 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h6><h6 id="-3"><a href="#-3" class="headerlink" title=" "></a> </h6><h6 id="-4"><a href="#-4" class="headerlink" title=" "></a> </h6><ol>
<li></li>
</ol>
]]></content>
      <categories>
        <category>聚类</category>
        <category>论文</category>
        <category>机器学习</category>
        <category>CV</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>阅读笔记210128：Double Self-weighted Multi-view Clustering via Adaptive View Fusion</title>
    <url>/2020/12/12/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0210128%EF%BC%9ADouble%20Self-weighted%20Multi-view%20Clustering%20via%20Adaptive%20View%20Fusion/</url>
    <content><![CDATA[<p><strong>总结：采用两步权重，即考虑了不同特征、不同视图对聚类的影响，通过权值自适应调整来聚合各种特征、不同视图，结果在3 Sources、ORL、BBC三个数据集上较AWP提升5%以上。# 总结：采用两步权重，即考虑了不同特征、不同视图对聚类的影响，通过权值自适应调整来聚合各种特征、不同视图，结果在3 Sources、ORL、BBC三个数据集上较AWP提升5%以上。</strong><br><a href="https://arxiv.org/abs/2011.10396" title="论文地址">论文地址</a><br><span id="more"></span></p>
<h3 id="关于AWP"><a href="#关于AWP" class="headerlink" title="关于AWP"></a>关于AWP</h3><p>本文是基于AWP(Multiview Clustering via Adaptively Weighted Procrustes)(本文第18参考文献)的改进，思路就是令<em>KR</em>趋近于真是的标签<em>Y</em>,那么关于AWP:</p>
<h4 id="Objective-function"><a href="#Objective-function" class="headerlink" title="Objective function"></a>Objective function</h4><script type="math/tex; mode=display">
\begin{array}{ll}
\min _{Y,\left\{R^{(i)}\right\}_{v}} & \sum_{i=1}^{v}\left\|Y-F^{(i)} R^{(i)}\right\|_{F} \\
\text { s.t. } & Y \in \text { Ind },\left(R^{(i)}\right)^{T} R^{(i)}=I, \forall i=1 \ldots v
\end{array}</script><h4 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h4><p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/01.png" alt="avatar"></p>
<h4 id="Datasets-and-Result"><a href="#Datasets-and-Result" class="headerlink" title="Datasets and Result"></a>Datasets and Result</h4><p>dataset:<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/02.png" alt="avatar"><br>Result:<br>事实上，提升的ACC最大也就在1%左右。<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/03.png" alt="avatar"></p>
<h3 id="Methodology："><a href="#Methodology：" class="headerlink" title="Methodology："></a>Methodology：</h3><h4 id="Objective-function-1"><a href="#Objective-function-1" class="headerlink" title="Objective function:"></a>Objective function:</h4><script type="math/tex; mode=display">
\begin{array}{l}
\min \sum_{v} w_{v}\left\|\left(M^{(v)}\right)^{1 / 2} \odot U^{(v)}\right\|_{F}^{2}+\frac{\mu}{2}\left\|M^{(v)}\right\|_{F}^{2} \\
w_{v}=\frac{1}{2\left(\left\|\left(M^{(v)}\right)^{1 / 2} \odot U^{(v)}\right\|_{F}\right)} \\
\text { s.t. } Y \in D e f,\left(R^{(v)}\right)^{T} R^{(v)}=I, S>0 \\
\left(M^{(v)}\right)^{T} E=E, U^{(v)}=Y-F^{(v)} R^{(v)}
\end{array}</script><p>其中<em>Y</em>是标签矩阵，<em>M</em>是特征权值矩阵，<em>F</em>是指示矩阵,<em>R</em>是旋转矩阵。</p>
<h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization:"></a>Optimization:</h4><p>采用交替更新的策略：<br>有拉格朗日方程：</p>
<script type="math/tex; mode=display">
J=\sum_{v}\left(w_{v}\left\|\left(M^{(v)}\right)^{1 / 2} \odot U^{(v)}\right\|_{F}^{2}+\frac{\mu}{2}\left\|M^{(v)}\right\|_{F}^{2}+\frac{\mu}{2}\left\|Y-F^{(v)} R^{(v)}-U^{(v)}+\frac{C^{(v)}}{\mu}\right\|_{F}^{2}\right)</script><p>根据拉格朗日方程可以求得多个参数的解。大部分求解求导为0就可以解决。</p>
<h4 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm:"></a>Algorithm:</h4><p>算法中的Eq.(11)等等就是参更新的解<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/04.png" alt="avatar"></p>
<h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets:"></a>Datasets:</h3><p>使用了3 Sources、ORL等6个数据集：<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/05.png" alt="avatar"></p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result:"></a>Result:</h3><p>重点关注ACC，可以看到在3 Sources和BBC，ORL等几个小的数据集上提升很大，在NUS等大一点的数据集上提升也就一般（很小）。<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/04/DSMC/06.png" alt="avatar"></p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h3><ol>
<li>在大小数据集上提升差异太大，考虑是否在大数据集上不太优越（当然也可能是特征数太少导致）；</li>
<li>确实如作者方法目的所言，在特征数更多的数据集上获得的提升更大，所以对于聚类任务，特征的筛选可能是很重要的。</li>
</ol>
]]></content>
      <categories>
        <category>聚类</category>
        <category>多视图学习</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>Pseudo-supervised Deep Subspace Clustering</title>
    <url>/2021/06/06/PSSC/</url>
    <content><![CDATA[<p>本文通过结合深度子空间聚类和自监督学习方法，提出了伪监督深度子空间聚类。使用伪标签为监督，充分利用关系信息来学习判别特征。<br><span id="more"></span><br>1013801202</p>
<script type="math/tex; mode=display">\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}</script><h5 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h5><p>借助于深度神经网络的强大的特征提取能力，基于自编码器的深度子空间聚类取得了令人印象深刻的表现。但是，自编码器的自我重建损失会忽略丰富的有用关系信息，并可能得到不具区分度的表示，从而不可避免地恶化聚类性能。 在不提供语义标签的情况下学习高级相似性也是一个挑战。另一个未解决的问题是由编码器和解码器之间的自表达层引起的高存储成本。为了解决这些问题，我们使用成对相似性来衡量重构损失以捕获局部结构信息而相似性是通过自表达层来学习的。伪图和伪标签可进一步利用网络训练期间获得的不确定知识，并用来监督相似性学习。通过使用联合学习和迭代训练有助于获得总体最佳解决方案。在基准数据集上进行的大量实验证明了我们方法的优越性。通过与k-最近邻算法相结合，我们进一步展示了我们的方法可以解决大规模和样本外问题。</p>
<p>鉴于深度学习强大的特征提取能力，基于深度学习的聚类算法受到了广泛的关注，此类方法将深度学习与传统聚类方法进行融合，很好地提升了聚类性能。自动编码器（AE）是许多现有的深度聚类的常用构建模块，本质上，自动编码器利用自我重构损失来找到潜在的表示形式，从而捕获原始数据的突出特征。但是现有的自动编码器仍然存在一些缺点，其仅使用数据点本身重构，而与其他数据点无关[1，2]。换句话说，由于忽略了相邻对象之间的丰富关系信息，潜在表示的质量可能会降低。理想情况下，所有数据点在重构损失中应具有不同的重要性，以反映其区分作用。因此，将聚类和重构直接结合起来是有问题的，因为前者旨在破坏不具区分度的细节，而后者则失去了具有区分度的信息[3]。</p>
<p>依靠子空间聚类的最新发展，深度子空间聚类引起了研究人员的关注。深度子空间聚类（DSC）网络[4]是DNN中子空间聚类模型的最直接实现。DSC在编码器和解码器之间添加了一个自表示层，以学习相似度图。在DSC的基础上，文献[5]进一步引入对抗训练以提高绩效。[6]也考虑了DSC在多视图和多模式数据中的应用。尽管取得了上述进展，但由于缺乏具体的监督，深度聚类仍然是一项艰巨的任务。</p>
<p>为了解决上述问题，我们提出了伪监督深度子空间聚类（PSSC），PSSC方法旨在通过提供伪标签来学习高级相似性。具体来说，我们的方法建立在自我训练的一般概念之上，其中模型经历了多次训练迭代。在每次迭代中，模型都使用来自上次迭代的预测来重新标记未标记的样本。我们在编码器模块上加了一个分类器，该分类器可以使用潜在表示和相似图构造伪标签来监督特征学习和图学习的训练。此外，为了保留局部结构并获得更多信息，我们采用加权重构损失代替了自编码器中广泛使用的自我重构损失。</p>
<h5 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h5><center><font size="">伪监督深度子空间聚类</center>

<center><em>吕俊成<sup>1</sup></em>, <em>康昭<sup>1</sup></em>, <em>陆啸<sup>1</sup></em>, <em>徐增林<sup>2</sup></em> </center>

<center><sup>1</sup>电子科技大学,   <sup>2</sup>哈尔滨工业大学（深圳)</center>
<center>TIP 2021</center>
<center>原文标题：Pseudo-supervised Deep Subspace Clustering</center>
<center>原文地址：https://ieeexplore.ieee.org/document/9440402</center>
<center>原文代码：https://github.com/sckangz/SelfsupervisedSC</center>


<h5 id="深度伪监督子空间聚类"><a href="#深度伪监督子空间聚类" class="headerlink" title="深度伪监督子空间聚类"></a>深度伪监督子空间聚类</h5><p>如图1所示，我们的伪监督深度子空间聚类网络分为三个部分：局部保留模块、自表达模块、伪监督模块。损失函数主要包括四个部分：带权重构损失、自表达约束、伪图损失和伪标签损失。<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/1.png" alt="avatar"></p>
<center><font size='2'>图1. 伪监督深度子空间聚类的框架结构</center>

<p>具体来说，原始数据$X$经过编码器到潜在表示$Z$, $Z$通过$ZC$来自表达，并通过解码器重构为$\widehat{X}$。而在重构损失时，局部保留模块采用带权重构损失函数而非自我重构。$Z$经过分类层得到$f_{\theta_s}(z)$。自表达模块学习到相似性图 $C$，用于带权重建，并与 $f_{\theta_s}(z)$一起生成伪监督信息用于监督整个网络的训练。</p>
<ol>
<li><h6 id="局部保留模块"><a href="#局部保留模块" class="headerlink" title="局部保留模块"></a>局部保留模块</h6><p>为了保留数据的局部信息，我们提出了带权重构损失函数。具体来说， $X_i$是由一组$X_j(i\neq j)$以权重$S_{ij}$来重建的，其中$S_{ij}$表示$X_i$和$X_j$的相似性。一方面，更远的样本将会对应更小的相似性，另一方面，更大的相似性可以使样本在恢复的过程中尽可能地接近，减小了离群点对模型带来的影响。公式表示为：</p>
<script type="math/tex; mode=display">
L_{0}=\sum_{i j} S_{i j}\left\|X_{i}-G_{\Theta_{d}}\left(F_{\Theta_{e}}\left(X_{j}\right)\right)\right\|^{2}</script><p>式（1）包含了数据点局部的结构信息，保证了样本在原始数据空间的局部结构信息能被保留到低维潜在空间。鉴于$S$是正定且对称的相似性矩阵, 上式还可转化为</p>
<script type="math/tex; mode=display">
\begin{aligned}
L_{0} &=\sum S_{i j}\left\|X_{i}-\hat{X}_{j}\right\|^{2} \\
&=\sum S_{i j}\left(\left\|X_{i}\right\|^{2}-2 X_{i}^{T} \hat{X}_{j}+\left\|\hat{X}_{j}\right\|^{2}\right) \\
&=\sum S_{i j}\left[\left(\left\|X_{i}\right\|^{2}-2 X_{i}^{T} \hat{X}_{i}+\left\|\hat{X}_{i}\right\|^{2}\right)\right.\\
&\left.+2\left(X_{i}^{T} \hat{X}_{i}-X_{i}^{T} \hat{X}_{j}\right)\right] \\
&=\operatorname{Tr}\left[(X-\hat{X})^{T} D(X-\hat{X})\right]+2 \operatorname{Tr}\left(X^{T} L \hat{X}\right)
\end{aligned}</script><p>其中, $D=Diag(\sum_{j=1}^n S_{ij})$为$S$的度矩阵，而拉普拉斯矩阵$L=D-S$。我们可以看到，相似度矩阵$S$的表现对于网络的性能至关重要。现有的研究者多使用预先固定的图结构[7]，而我们则尝试基于自我表达的性质从数据中自动学习$S$</p>
</li>
<li><p>自表达模块</p>
<p>与深度子空间聚类一样，我们的自我表达模块也在编码器和解码器之间添加了一层没有偏置的完全连接层，其权重代表系数矩阵$C$,矩阵$C$可以表示数据的子空间结构，即如果第i个样本和第j个样本不在同一子空间中，则$C_{ij}=0$。为了学习适合子空间聚类的特征表示，我们的自我表达模块的损失函数为</p>
<script type="math/tex; mode=display">
L_{1}=\alpha\|C\|_{p}+\frac{1}{2}\|Z-Z C\|_{F}^{2} \quad \text { s.t. } \quad \operatorname{diag}(C)=0</script><p>其中第一项是$C$上的某个正则化函数，第二项使潜在空间$Z$中的重构误差最小。一旦获得了自表达系数矩阵$C$，我们令$S=\frac{1}{2}(|C|+|C|^{\top})$，建立起了重构相似图$S$与自表达系数$C$的联系。由于得到的相似性矩阵的行列所用尺度可能不同，因此我们使用对称归一化拉普拉斯算子进行尺度归一化，同时保持相似矩阵$S$的对称性。具体地说，正则化拉普拉斯矩阵$L_{n}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$，这样就可以得到$Tr[(X-\widehat{X})^{\top}D^n(X-\widehat{X})]=|X-\widehat{X}|^2_F$。所以，我们的加权重构的损失函数最终转换如下：</p>
<script type="math/tex; mode=display">
L_{0}=\|X-\hat{X}\|_{F}^{2}+2 \operatorname{Tr}\left(X^{T} L_{n} \hat{X}\right)</script><p>由于的第二项包含了对$C$的约束，所以我们去掉了$L_1$的第一项</p>
</li>
<li><p>伪监督模块</p>
<p>我们在编码器后面加上了一个以softmax 函数做激活函数的全连接层，将提取到的特征表示$Z$转化为$f_{\theta_s}(z)$，其中$f_{\theta_s}(z) \in \mathbb{R}^K$表示对$z_i$的特征预测， K代表聚类的数量。所以具有以下性质：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\sum_{t=1}^{K} f_{\theta_{s}}\left(z_{i}\right)_{t}=1, \forall i=1, \ldots, n \\
&f_{\theta_{s}}\left(z_{i}\right)_{t} \geq 0, \forall t=1, \ldots, K
\end{aligned}</script><p>此外， $f_{\theta_s}(z)$的分布应与我们先前学习到的相似性图$S$一致。因为$S$是在训练过程中学习的，它本质上是伪图。基于此我们提出了伪图监督损失如下：</p>
<script type="math/tex; mode=display">
L_{g r a p h}=\sum_{z_{i}, z_{j} \in Z} \mathcal{L}_{g}\left(f_{\theta_{s}\left(z_{i}\right)}, f_{\left.\theta_{s\left(z_{j}\right.}\right)} ; S_{i j}\right)</script><p>其中$\mathcal{L}_g$为计算距离的损失函数，其常见的表达形式包括对比暹罗网络损失和二值交叉熵等。</p>
<p>理想情况下，我们的伪图应具有$K$个连通的组件或分区，而这$K$个分区可以自然地被视为伪标签。如此以来，伪图监督的最优解将会使预测向量变成独热形式, 该预测作为伪标签可表示为：</p>
<script type="math/tex; mode=display">
y_{i}=\arg \max _{k}\left[f_{\theta_{s}^{*}}\left(z_{i}\right)\right]_{k}</script><p>其中，  $[\cdot]_k$代表预测向量的第$k$个元素。可以很容易地得到，其对应的预测伪标签概率为$p_i=max[f_{\theta_{s}}(z_i)]$。</p>
<p>然而在现实训练中，由于具有非凸性，是很难获得最优解，因此，预测向量$f_{\theta_{s}}(z_i)$也不严格具有独热形式，为了解决这个问题，我们为概率$p_i$设置了一个较大的阈值 ，以选择高可置信的伪标签进行监督：</p>
<script type="math/tex; mode=display">
V_{i}=\left\{\begin{array}{ll}
1 & \text { if } p_{i} \geq \text { thres } \\
0 & \text { otherwise. }
\end{array}\right.</script><p>这样以来，只有具有高可置信度伪标签的样本才能参与网络训练，提高网络 训练的质量。因此，伪标签监督损失可以表述为：</p>
<script type="math/tex; mode=display">
L_{\text {label }}=\sum_{i=1}^{n} V_{i} * \mathcal{L}_{l}\left(f_{\theta_{s}\left(z_{i}\right)}, y_{i}\right)</script><p>其中，损失函数$\mathcal{L}_1$为交叉熵函数。</p>
</li>
<li><p>联合优化训练</p>
<p>为了获得统一的框架并共同训练网络，我们将三个模块的损失函数统一，得到PSSC的最终损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(\Theta)=&\left\|X-\hat{X}_{\Theta}\right\|_{F}^{2}+2 \operatorname{Tr}\left(X^{T} L_{n} \hat{X}_{\Theta}\right) \\
&+\gamma_{1}\left\|Z_{\Theta_{e}}-Z_{\Theta_{e}} C\right\|_{F}^{2}+\gamma_{2} L_{\text {graph }}+\gamma_{3} L_{\text {label }} \\
& \text { s.t. } \operatorname{diag}(C)=0
\end{aligned}</script><p>其中$\gamma_1,\gamma_2,\gamma_3$是权衡参数。我们选择了卷积神经网络来实现上面的网络框架，训练时使用反向传播算法。当网络框架训练完成后，我们将获得较低维表示$Z$和相似性矩阵$C$。</p>
<h5 id="子空间实验"><a href="#子空间实验" class="headerlink" title="子空间实验"></a>子空间实验</h5><p>我们在五个图像数据集上进行了聚类实验，包括MNIST，ORL，COIL20，COIL40，Umist，统计信息如表1。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/2.png" alt="avatar"></p>
</li>
</ol>
<center><font size='2'>表1. 五个基准图像数据集的统计信息</center>

<p>我们采用了三种常用的聚类结果评价指标，包括聚类准确率（ACC）、归一化互信息（NMI)和纯度（PUR）。具体结果如表2所示，我们可以看出，在五个数据集上三个不同评测指标下，所提出的PSSC均显著超过了其他对比方法。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/11.png" alt="avatar"></p>
<center><font size='2'>表2. 基准数据集上的聚类结果</center>

<p>图2展示了我们的方法和DSC在MNIST上重构的图像，第一行是原始图像，第二行是我们的PSSC重构的图像，第三行是DSC。我们发现PSSC可以很好地恢复边缘，而DSC的图像却模糊了。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/3.png" alt="avatar"></p>
<center><font size='2'>图2. MNIST的重构图像。从上到下：原始图像，PSSC，DSC</center>

<h5 id="大规模数据集"><a href="#大规模数据集" class="headerlink" title="大规模数据集"></a>大规模数据集</h5><p>之前提到过由于自表达层的结构是$n\times n$的矩阵，导致DSC具有巨大的存储成本，这阻碍了它在大规模数据集上的应用[8]。这解释了为什么所有DSC论文都使用小型数据集。我们提出了一种缓解此问题的方法，并在大规模数据集上进行实验。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/4.png" alt="avatar"></p>
<center><font size='2'>图3. 大规模数据集下伪监督深度子空间聚类网络的结构</center>

<p>如图3，我们分两步进行大规模数据集实验。首先，抽取一小批次样本 为核心点，用以训练PSSC网络，如黄色框内所示，得到潜在空间表示 ，并将聚类结果作为标签 。然后，将剩余样本点 编码到潜在空间表示 ，用最近邻分类器找到与 最近的核心点 ，并为其分配标签。通过这种方法，我们的方法可以解决样本外问题，这是子空间聚类的长期挑战。</p>
<p>我们在三个数据集上进行了实验，包括两个图像数据集：MNIST和USPS；一个文字数据集RCV1。三个数据集统计信息如表3。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/5.png" alt="avatar"></p>
<center><font size='2'>表3. 大规模数据集统计信息</center>

<p>实验结果在表4，可以看出，我们的方法仍然优于最近的深度聚类方法。<br><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/6.png" alt="avatar"></p>
<center><font size='2'>表4. 大规模数据集上聚类结果对比</center>

<p>为了展现在潜在空间下的数据分布，以USPS数据为例，我们使用 t-SNE方法来可视化对比方法所学习到的潜在空间表示。图5展示了六种方法使用t-SNE可视化后的结果。不同的颜色代表不同的数字。同一颜色的样本彼此越聚集，并且不同颜色的样本距离越远，则潜在空间的表示越好。</p>
<p><img src="https://pel-img-1301311422.cos.ap-chengdu.myqcloud.com/blog/blog_img/2021/06/PSSC/7.png" alt="avatar"></p>
<center><font size='2'>图4. 在 USPS 数据集上学习的潜在空间的 2D 可视化
结论</center>

<h5 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h5><p>本文通过结合深度子空间聚类和自监督学习方法，提出了伪监督深度子空间聚类。使用伪标签为监督，充分利用关系信息来学习判别特征。首次将局部信息保留、子空间自表达和自监督整合到一个统一的框架。在小规模和大规模数据集上的大量实验结果表明，所提出的方法在相似性和表示学习方面优于最新的方法。特别地，我们展示了如何解决子空间聚类所面临的大规模和样本外问题。</p>
<h5 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h5><p>[1] S. Wang, Z. Ding, and Y. Fu, Feature selection guided auto-encoder, in Thirty-First AAAI Conference on Artificial Intelligence, 2017.</p>
<p>[2] Z. Kang, X. Lu, J. Liang, K. Bai, and Z. Xu, Relation-guided representation learning, Neural Networks, 2020, Vol. 131, pp. 93–102.</p>
<p>[3] N. Mrabah, N. M. Khan, R. Ksantini, and Z. Lachiri, Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction, Neural Networks, 2020, Vol. 130, pp. 206-228. </p>
<p>[4] P. Ji, T. Zhang, H. Li, M. Salzmann, and I. Reid, Deep subspace clustering networks, in Advances in Neural Information Processing Systems, 2017, pp. 24–33.</p>
<p>[5] P. Zhou, Y. Hou, and J. Feng, Deep adversarial subspace clustering, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 1596–1604.</p>
<p>[6] M. Abavisani and V. M. Patel, Deep multimodal subspace clustering networks, IEEE Journal of Selected Topics in Signal Processing, 2018, 12(6): 1601–1614. </p>
<p>[7] Z. Kang, H. Pan, S. C. H. Hoi, and Z. Xu. Robust graph learning from noisy data. IEEE Transactions on Cybernetics, 2020, 50(5): 1833-1843.</p>
<p>[8] Structured Graph Learning for Scalable Subspace Clustering: From Single-view to Multi-view. IEEE Transactions on Cybernetics, 2021.<br>转自 中国图象图形学学会CSIG 微信公众号文章：<a href="https://mp.weixin.qq.com/s/OJV3kpFcNVbpT6zRWYLDyQ">https://mp.weixin.qq.com/s/OJV3kpFcNVbpT6zRWYLDyQ</a></p>
]]></content>
      <categories>
        <category>聚类</category>
      </categories>
      <tags>
        <tag>聚类</tag>
      </tags>
  </entry>
</search>
